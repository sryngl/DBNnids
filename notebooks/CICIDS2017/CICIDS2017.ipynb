{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa61717e",
   "metadata": {},
   "source": [
    "# CICIDS2017 dataset\n",
    "------------------------------------------------\n",
    "\n",
    "<div>\n",
    "    <b>Aim</b>: This notebook provide you with a way to create a clean CICIDS2017 dataset using all the .csv files.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "    <b>Dataset description</b>: The CICIDS2017 dataset, which has been created by the <a href=\"https://www.unb.ca/cic/datasets/ids-2017.html\">Canadian Institute for Cyber-security (CIC)</a>, consists of labeled network flows. The CICIDS2017 contains benign and the most up-to-date common attacks. It is made up of 2,830,743 records with a total of 78 features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79466da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bc6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", color_codes=True)\n",
    "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f6c28",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54c8365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory C:\\Users\\tangy\\Desktop\\Files\\信安赛\\projects\\dbn-based-nids-master\\data exists.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR  = os.path.join(os.path.abspath(\"../..\"), \"data\")\n",
    "IMAGE_DIR = os.path.join(os.path.abspath(\"../..\"), \"images\")\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"The directory {DATA_DIR} exists.\")\n",
    "else:\n",
    "    print(f\"The directory {DATA_DIR} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84d016",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621908c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(column):\n",
    "    column = column.strip(' ')\n",
    "    column = column.replace('/', '_')\n",
    "    column = column.replace(' ', '_')\n",
    "    column = column.lower()\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae17d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the .csv files\n",
    "filenames = glob.glob(os.path.join(DATA_DIR, 'raw', '*.csv'))\n",
    "datasets = [pd.read_csv(filename) for filename in filenames]\n",
    "\n",
    "# Remove white spaces and rename the columns\n",
    "for dataset in datasets:\n",
    "    dataset.columns = [clean_column_name(column) for column in dataset.columns]\n",
    "\n",
    "# Concatenate the datasets\n",
    "dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "dataset.drop(labels=['fwd_header_length.1'], axis= 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bbcd4",
   "metadata": {},
   "source": [
    "Have an initial inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31659aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c77672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include=[int, float])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include=[object]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18933890",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfbf3e",
   "metadata": {},
   "source": [
    "### Dealing with duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379b235",
   "metadata": {},
   "source": [
    "We first check if there are duplicates after combining the eight .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92696a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a50a3",
   "metadata": {},
   "source": [
    "As we can see from above, there are duplicates and we need get rid of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size BEFORE deleteting instances with duplicate values: ', dataset.shape[0], end='\\n\\n')\n",
    "\n",
    "# Remove duplicate rows\n",
    "dataset.drop_duplicates(inplace=True, keep=False, ignore_index=True)\n",
    "\n",
    "print('Data size AFTER deleteting instances containing duplicate values: ', dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29097e80",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d47669",
   "metadata": {},
   "source": [
    "Then, we check if there are missing values in each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5395d",
   "metadata": {},
   "source": [
    "As we can see from above, there are 320 missing values in this **CIC-IDS-2017** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c0c2a",
   "metadata": {},
   "source": [
    "We have a few options to deal with missing values:\n",
    "1. Get rid of the corresponding instances.\n",
    "2. Get rid of the whole attribute.\n",
    "3. Set the values to some value (zero, the mean, the median, etc.).\n",
    "4. Use imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum() / dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bb747",
   "metadata": {},
   "source": [
    "The number of instances in the dataset is large enough and the fraction of intances with missing values is small, an easy way is simply to remove those instances containing missing values. However, we need to first ensure that the missing values are not related to a specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns[dataset.isnull().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e1b7a",
   "metadata": {},
   "source": [
    "All the missing values come from the `flow_bytes_s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3aacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size BEFORE deleteting instances with missing values: ', dataset.shape[0], end='\\n\\n')\n",
    "\n",
    "# Remove missing values\n",
    "dataset.dropna(axis=0, inplace=True, how=\"any\")\n",
    "\n",
    "print('Data size AFTER deleteting instances containing missing values: ', dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e3a4b",
   "metadata": {},
   "source": [
    "### Dealing with infinite values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03aa00",
   "metadata": {},
   "source": [
    "Checking if all values are finite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isfinite(dataset.drop(['label'], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values to NaN\n",
    "dataset.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Check which labels are related to infinte values\n",
    "dataset[(dataset['flow_bytes_s'].isnull()) & (dataset['flow_packets_s'].isnull())].label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size BEFORE deleteting instances with infinite values: ', dataset.shape[0], end='\\n\\n')\n",
    "\n",
    "# Remove infinte values\n",
    "dataset.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "print('Data size AFTER deleteting instances containing infinite values: ', dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3b186",
   "metadata": {},
   "source": [
    "### Dealing with features with quasi null std deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4650ca6",
   "metadata": {},
   "source": [
    "Standard deviation denoted by sigma (σ) is the average of the squared root differences from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_std = dataset.std(numeric_only=True)\n",
    "dataset_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Features that meet the threshold\n",
    "constant_features = [column for column, std in dataset_std.iteritems() if std < 0.01]\n",
    "\n",
    "# Drop the constant features\n",
    "dataset.drop(labels=constant_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6035350",
   "metadata": {},
   "source": [
    "Some features such as `bwd_psh_flags`, `fwd_urg_flags`, `bwd_urg_flags`, `cwe_flag_count`, `fwd_avg_bytes_bulk`, `fwd_avg_packets_bulk`, `fwd_avg_bulk_rate`, `bwd_avg_bytes_bulk`, `bwd_avg_packets_bulk`, `bwd_avg_bulk_rate`  don't vary. Hence, the correlation is NaN by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45c0a5",
   "metadata": {},
   "source": [
    "### Observing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db51ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = dataset.quantile(0.25)\n",
    "Q3 = dataset.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifying outliers with interquartile range\n",
    "filt = (dataset < (Q1 - 1.5 * IQR)) | (dataset > (Q3 + 1.5 * IQR))\n",
    "print(filt.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=dataset[[\"average_packet_size\", \"avg_bwd_segment_size\"]], orient=\"h\")\n",
    "\n",
    "#plt.title('Summary of some variables containing outliers', fontsize=18)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(IMAGE_DIR, 'outliers.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22005f",
   "metadata": {},
   "source": [
    "Convert the dtype of some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['flow_bytes_s', 'flow_packets_s']] = dataset[['flow_bytes_s', 'flow_packets_s']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a549209",
   "metadata": {},
   "source": [
    "Create a new feature `Port Category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "conds = [\n",
    "    (dataset['destination_port'] >= 1) & (dataset['destination_port'] < 1024),\n",
    "    (dataset['destination_port'] >= 1024) & (dataset['destination_port'] < 49152),\n",
    "    (dataset['destination_port'] >= 49152) & (dataset['destination_port'] <= 65535)\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    \"1 - 1023\", \n",
    "    \"1024 - 49151\",\n",
    "    \"49152 - 65535\"\n",
    "]\n",
    "\n",
    "dataset.insert(1, 'destination_port_category', np.select(conds, choices, default=\"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40097c",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81af80",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c19945",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_corr = dataset.corr()\n",
    "dataset_corr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "sns.set(font_scale=1.0)\n",
    "ax = sns.heatmap(dataset_corr, annot=False)\n",
    "fig.savefig(os.path.join(IMAGE_DIR, 'correlation matrix.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b46d1a",
   "metadata": {},
   "source": [
    "We can see that some features seems to be highly correlated. Hence, we might need to remove them since there are bringing redundant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create & Apply mask\n",
    "mask = np.triu(np.ones_like(dataset_corr, dtype=bool))\n",
    "tri_df = dataset_corr.mask(mask)\n",
    "\n",
    "# Find Features that meet the threshold\n",
    "correlated_features = [c for c in tri_df.columns if any(tri_df[c] > 0.98)]\n",
    "\n",
    "# Drop the highly correlated features\n",
    "dataset.drop(labels=correlated_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed852d",
   "metadata": {},
   "source": [
    "### Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fcee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "attack = dataset['label'].value_counts()\n",
    "\n",
    "attack_count = attack.values\n",
    "attack_type = attack.index\n",
    "\n",
    "bar = plt.bar(attack_type, attack_count, align='center')\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, format(height, ','), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Distribution of different type of network activity in the dataset')\n",
    "plt.xlabel('Network activity')\n",
    "plt.ylabel('Number of instances')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(IMAGE_DIR, 'network_activity.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e05822",
   "metadata": {},
   "source": [
    "The dataset is clearly imbalaced and we need to deal with it. We can merge few minority classes having similar characteristics and behavior to form new attack classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7defce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'] = dataset['label'].str.replace('Web Attack �', 'Web Attack', regex=False)\n",
    "\n",
    "# Proposed Groupings\n",
    "attack_group = {\n",
    "    'BENIGN': 'Benign',\n",
    "    'PortScan': 'PortScan',\n",
    "    'DDoS': 'DoS/DDoS',\n",
    "    'DoS Hulk': 'DoS/DDoS',\n",
    "    'DoS GoldenEye': 'DoS/DDoS',\n",
    "    'DoS slowloris': 'DoS/DDoS', \n",
    "    'DoS Slowhttptest': 'DoS/DDoS',\n",
    "    'Heartbleed': 'DoS/DDoS',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'Bot': 'Botnet ARES',\n",
    "    'Web Attack Brute Force': 'Web Attack',\n",
    "    'Web Attack Sql Injection': 'Web Attack',\n",
    "    'Web Attack XSS': 'Web Attack',\n",
    "    'Infiltration': 'Infiltration'\n",
    "}\n",
    "\n",
    "# Create grouped label column\n",
    "dataset['label_category'] = dataset['label'].map(lambda x: attack_group[x])\n",
    "dataset['label_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ddcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "attack = dataset['label_category'].value_counts()\n",
    "\n",
    "attack_count = attack.values\n",
    "attack_type = attack.index\n",
    "\n",
    "bar = plt.bar(attack_type, attack_count, align='center')\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, format(height, ','), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.title('Distribution of different type of network activity in the dataset', fontsize=18)\n",
    "plt.xlabel('Network activity', fontsize=16)\n",
    "plt.ylabel('Number of instances', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(IMAGE_DIR, 'network_activity_category.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb12ad3",
   "metadata": {},
   "source": [
    "### Port Usage Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e379aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "benign_ports = dataset.loc[dataset['label'] == 'BENIGN', 'destination_port_category']\n",
    "malicious_ports = dataset.loc[dataset['label'] != 'BENIGN', 'destination_port_category']\n",
    "\n",
    "# get rid of rows with specific value\n",
    "benign_ports = benign_ports[benign_ports != \"0\"]\n",
    "malicious_ports = malicious_ports[malicious_ports != \"0\"]\n",
    "\n",
    "# sum each port category column\n",
    "benign_ports = benign_ports.value_counts()\n",
    "malicious_ports = malicious_ports.value_counts()\n",
    "\n",
    "indexes = np.arange(3)\n",
    "width = 0.4\n",
    "rect1 = plt.bar(indexes, benign_ports.values, width, color=\"steelblue\", label=\"benign\")\n",
    "rect2 = plt.bar(indexes + width, malicious_ports.values, width, color=\"indianred\", label=\"malicious\")\n",
    "\n",
    "def add_text(rect):\n",
    "    # add text to top of each bar\n",
    "    for r in rect:\n",
    "        h = r.get_height()\n",
    "        plt.text(r.get_x() + r.get_width()/2, h*1.01, s=format(h, \",\") ,fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "add_text(rect1)\n",
    "add_text(rect2)\n",
    "\n",
    "ax.set_xticks(indexes + width / 2)\n",
    "ax.set_xticklabels([\"1 - 1,023\", \"1,024 - 49,151\", \"49,152 - 65,535\"])\n",
    "plt.title('Distribution of Port Usage\\nAccording to Network Activity Type')\n",
    "plt.xlabel('Port Range')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(IMAGE_DIR, 'port_usage_comparison.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67a494",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442fc77",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a64248",
   "metadata": {},
   "source": [
    "First step to data preparation is splitting the data into traning and testing sets. For this there already exists sklearn function that does all the splitting for us. This step is important so we can have representative data for evaluating our model. Both train and test samples should contain similar data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['label_category']\n",
    "features = dataset.drop(labels=['label', 'label_category', 'destination_port_category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdff883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42, stratify=labels)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728403c",
   "metadata": {},
   "source": [
    "### Scaling features to a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98336b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = features.select_dtypes(exclude=[\"int64\", \"float64\"]).columns\n",
    "numeric_features = features.select_dtypes(exclude=[object]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('categoricals', OneHotEncoder(drop='first', handle_unknown='error'), categorical_features),\n",
    "    ('numericals', QuantileTransformer(), numeric_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c90bb",
   "metadata": {},
   "source": [
    "Preprocess the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric_features.tolist()\n",
    "\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train), columns=columns)\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test), columns=columns)\n",
    "X_val = pd.DataFrame(preprocessor.transform(X_val), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554d9d4",
   "metadata": {},
   "source": [
    "Preprocess the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb40ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train = pd.DataFrame(le.fit_transform(y_train), columns=[\"label\"])\n",
    "y_test = pd.DataFrame(le.transform(y_test), columns=[\"label\"])\n",
    "y_val = pd.DataFrame(le.transform(y_val), columns=[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9ef07",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_features.pkl'))\n",
    "X_val.to_pickle(os.path.join(DATA_DIR, 'processed', 'val/val_features.pkl'))\n",
    "X_test.to_pickle(os.path.join(DATA_DIR, 'processed', 'test/test_features.pkl'))\n",
    "\n",
    "y_train.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_labels.pkl'))\n",
    "y_val.to_pickle(os.path.join(DATA_DIR, 'processed', 'val/val_labels.pkl'))\n",
    "y_test.to_pickle(os.path.join(DATA_DIR, 'processed', 'test/test_labels.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def balance_dataset(X, y, undersampling_strategy, oversampling_strategy):\n",
    "\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=0)\n",
    "    X_under, y_under = under_sampler.fit_resample(X, y)\n",
    "\n",
    "    over_sampler = SMOTE(sampling_strategy=oversampling_strategy)\n",
    "    X_bal, y_bal = over_sampler.fit_resample(X_under, y_under)\n",
    "    \n",
    "    return X_bal, y_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad16b0",
   "metadata": {},
   "source": [
    "## Balance the training set using combination of `SMOTE` & `RandomUnderSampler`\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7370f",
   "metadata": {},
   "source": [
    "***Label Encoder Transformation***\n",
    "```json\n",
    "{\n",
    "    'Benign': 0,\n",
    "    'DoS/DDoS': 3,\n",
    "    'PortScan': 4,\n",
    "    'Brute Force': 2,\n",
    "    'Web Attack': 5,\n",
    "    'Botnet ARES': 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampling_strategy = {\n",
    "    0: 800000,\n",
    "    3: 192161,\n",
    "    4: 34383,\n",
    "    2: 5131,\n",
    "    5: 1271,\n",
    "    1: 1166,\n",
    "}\n",
    "\n",
    "oversampling_strategy = {\n",
    "    0: 800000,\n",
    "    3: 212102,\n",
    "    4: 44460,\n",
    "    2: 50115,\n",
    "    5: 50284,\n",
    "    1: 50149,\n",
    "}\n",
    "\n",
    "# Balance the training set\n",
    "X_train_bal, y_train_bal = balance_dataset(X_train, y_train, undersampling_strategy, oversampling_strategy)\n",
    "\n",
    "# Save the balanced training set\n",
    "X_train_bal.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_features_balanced.pkl'))\n",
    "y_train_bal.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_labels_balanced.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196ac2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# sum each port category column\n",
    "imbalanced = y_train.value_counts()\n",
    "balanced = y_train_bal.value_counts()\n",
    "\n",
    "indexes = np.arange(6)\n",
    "width = 0.4\n",
    "rect1 = plt.bar(indexes, imbalanced.values, width, color=\"steelblue\", label=\"imbalanced\")\n",
    "rect2 = plt.bar(indexes + width, balanced.values, width, color=\"indianred\", label=\"balanced\")\n",
    "\n",
    "def add_text(rect):\n",
    "    \"\"\"Add text to top of each bar.\"\"\"\n",
    "    for r in rect:\n",
    "        h = r.get_height()\n",
    "        plt.text(r.get_x() + r.get_width()/2, h*1.01, s=format(h, \",\") ,fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "add_text(rect1)\n",
    "add_text(rect2)\n",
    "\n",
    "ax.set_xticks(indexes + width / 2)\n",
    "ax.set_xticklabels(['Benign', 'DoS/DDoS', 'PortScan', 'Brute Force', 'Web Attack', 'Botnet ARES'])\n",
    "plt.xlabel('Traffic Activity', fontsize=16)\n",
    "plt.ylabel('# instances', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(IMAGE_DIR, 'balanced_dataset.pdf'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
